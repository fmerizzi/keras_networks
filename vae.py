# -*- coding: utf-8 -*-
"""vae.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18vLFf_KOeNZAlF9wrk6H3sTb4wLdqy0G

In this notebook we shall present a simple conditional VAE, trained on MNIST
"""

import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras import Model
from tensorflow.keras import backend as K
from tensorflow.keras import metrics
from tensorflow.keras import utils
from keras.datasets import mnist
from keras.datasets import fashion_mnist
from keras.datasets import cifar10
import numpy as np
import matplotlib.pyplot as plt

"""The conditional autoencoder will allow to generate specific digits in the MNIST range 0-9. The condition is passed as input to encoder and decoder in categorical format."""

# train the VAE on MNIST digits
(x_train, y_train), (x_test, y_test) = cifar10.load_data()

x_train = x_train.astype('float32') / 255.
x_test = x_test.astype('float32') / 255.
x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))
x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))
y_train = utils.to_categorical(y_train)
y_test = utils.to_categorical(y_test)

print(x_train.shape)

"""# The model

Sampling function for the Variational Autoencoder.
This is the clsed form of the Kullback-Leibler distance between a gaussian N(z_mean,z_var) and a normal prior N(0,1)
"""

def sampling(args):
    z_mean, z_log_var = args
    epsilon = K.random_normal(shape=(K.shape(z_mean)[0], latent_dim), mean=0.,
                              stddev=1.)
    return z_mean + K.exp(z_log_var / 2) * epsilon

x_train.shape

"""Main dimensions for the model (a simple stack of dense layers)."""

input_dim = 3072
latent_dim = 128
inter_two_dim = 256
inter_one_dim = 512

"""We start with the encoder. It takes two inputs: the image and the category.

It returns the latent encoding (z_mean) and a (log-)variance for each latent variable.
"""

x = layers.Input(shape=(input_dim,))
y = layers.Input(shape=(10,))
xy = layers.concatenate([x,y])
h0 = layers.Dense(inter_one_dim, activation='relu')(xy)
h1 = layers.Dense(inter_one_dim, activation='relu')(h0)
h2 = layers.Dense(inter_two_dim, activation='relu')(h1)
h = layers.Dense(inter_two_dim, activation='relu')(h2)
z_mean = layers.Dense(latent_dim)(h)
z_log_var = layers.Dense(latent_dim)(h)

"""Now we sample around z_mean with the associated variance. 

Note the use of the "lambda" layer to transform the sampling function into a keras layer.
"""

z = layers.Lambda(sampling, output_shape=(latent_dim,))([z_mean, z_log_var])

"""Now we need to address the decoder. We first define its layers, in order to use them both in the vae model and in the stand-alone generator."""

decoder_mid = layers.Dense(inter_two_dim, activation='relu')
decoder_mid2 = layers.Dense(inter_two_dim, activation='relu')
decoder_mid3 = layers.Dense(inter_one_dim, activation='relu')
decoder_mid4 = layers.Dense(inter_one_dim, activation='relu')
decoder_out = layers.Dense(input_dim, activation='sigmoid')

"""We decode the image starting from the latent representation z and its category y, that must be concatenated."""

zy = layers.concatenate([z,y])
dec_mid = decoder_mid(zy)
dec_mid2 = decoder_mid2(dec_mid)
dec_mid3 = decoder_mid3(dec_mid2)
dec_mid4 = decoder_mid4(dec_mid3)
x_hat = decoder_out(dec_mid4)

vae = Model(inputs=[x,y], outputs=[x_hat])

"""Some hyperparameters. Gamma is used to balance loglikelihood and KL-divergence in the loss function"""

batch_size = 50
epochs = 100
gamma = 0.02

"""The VAE loss function is just the sum between the reconstruction error (mse or bce) and the KL-divergence, acting as a regularizer of the latent space."""

rec_loss = input_dim * metrics.binary_crossentropy(x, x_hat)
kl_loss = - 0.5 * K.sum(1 + z_log_var - K.square(z_mean) - K.exp(z_log_var), axis=-1)
vae_loss = K.mean(rec_loss + gamma*kl_loss)
vae.add_loss(vae_loss)

"""We are ready to compile. There is no need to specify the loss function, since we already added it to the model with add_loss."""

vae.compile(optimizer='adam')

vae.summary()

"""Train for a sufficient amount of epochs. Generation is a more complex task than classification."""

vae.fit([x_train,y_train], shuffle=True, epochs=epochs, batch_size=batch_size, validation_data=([x_test,y_test], None))

vae.save_weights("cvae256_8.h5")

"""Let us decode the full training set."""

decoded_imgs = vae.predict([x_test,y_test])

"""The following function is to test the quality of reconstructions (not particularly good, since compression is strong)."""

def plot(n=10):
  plt.figure(figsize=(20, 4))
  for i in range(n):
    # display original
    ax = plt.subplot(2, n, i + 1)
    plt.imshow(x_test[i].reshape(32, 32,3))
    #plt.gray()
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)

    # display reconstruction
    ax = plt.subplot(2, n, i + 1 + n)
    plt.imshow(decoded_imgs[i].reshape(32, 32,3))
    plt.gray()
    ax.get_xaxis().set_visible(False)
    ax.get_yaxis().set_visible(False)
  plt.show()

plot()

"""Finally, we build a digit generator that can sample from the learned distribution"""

noise = layers.Input(shape=(latent_dim,))
label = layers.Input(shape=(10,))
xy = layers.concatenate([noise,label])
dec_mid = decoder_mid(xy)
dec_mid2 = decoder_mid2(dec_mid)
dec_mid3 = decoder_mid3(dec_mid2)
dec_mid4 = decoder_mid4(dec_mid3)
dec_out = decoder_out(dec_mid4)
generator = Model([noise,label],dec_out)

"""And we can generate our samples"""

import time
# display a 2D manifold of the digits
n = 3  # figure with 15x15 digits
digit_size = 32
figure = np.zeros((digit_size * n, digit_size * n,3))

for j in range(10):
  print("Label : ", j)
  label = np.expand_dims(utils.to_categorical(j,10),axis=0)
  for i in range(0,n):
    for j in range (0,n):
        z_sample = np.expand_dims(np.random.normal(size=latent_dim),axis=0)
        x_decoded = generator.predict([z_sample,label])
        digit = x_decoded[0].reshape(digit_size, digit_size,3)
        figure[i * digit_size: (i + 1) * digit_size,
               j * digit_size: (j + 1) * digit_size] = digit
  plt.figure(figsize=(15, 15))
  plt.imshow(figure, cmap='Greys_r')
  plt.show()
  time.sleep(1)